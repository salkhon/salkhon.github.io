{"data":{"featured":{"edges":[{"node":{"frontmatter":{"pos":"Proceedings of the 2024 Conference On Language Modeling: COLM","title":"IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAC7klEQVQ4y3WU228bVRDG/R8i3hCCEKcVqWLRxKRSRYtoeGjKLagFkZaoJKahArUl9KUSAqmJEIEITK52Eu869tpr7669F+/Nl9ZxfuisamSn4ZPOas9l5sw338yJBa5DsahQqVRQVRVFUfA8l3bnGNttUSqVor1yuYznefRabepGDVmWMQwjWhd21WqVZrNJ7FkYYllWdFgMx3EIg5Bur0u72472hIHj2HQ6HaBHM/Sp1erRxcLGNE1arRYCMU4hCAJ0XadWq+HYDt1uF8/1oz3DqLG9s4uilDgLJycnxMSnPwSOj4+j21zXpaKKNChUjBKarrKf3eXw4IB2uzPkZNA+NjgpFoto1SrtTgdV05AK20i5daQ/f0bae0p6+1dK5Szd589RXuT2tNOhCEUudEOnrJRRMlsU//iJzTvfcDdxheXLs+ysPEDOPuUwl0bXdBpuY8jZfw4HYTY8tH82Mb+/TebGlyyNJ1m4+AZz8dd4kLxIafk2irRBw3OHaPcRiSKUkiQJWZLJlyWyv/zA6gcfMffWV3yRSJCaHGEhMcrd5Ahfv32F1cXfKap1At/F94Mh2rG+EJGyusGevMbi/Ce89+o15iYm+DZ5jtT0OEtT50lNjTHz+iW+m11DPjLxvQZBEA47HBSl1+txJKV5mFpgfvJ9Ph29yr3kKItT4yy/O8rNcxM8+WyZYiEdqX4m5f7Etm0qlSp5aZO1xyvMT89w/c1rLE2NkZqMk0qOcWc8zvaTx+SO1slLu1i2Q7N5RoQConUkSca0DOTcb8zPfM70K1e5eeESqXfi3EuMcCseZ/XRQ/KFv7AtE9OyCMPw5RyeVlrXC2ysr3H/1n1unJ/j44nrzF7+kJWVHznY3cCy6y91yBDlwTxms1n29w/wfZ+6WWdn65CtTI69ozyOW6esqqT/TpPJZF709qnCHlwQEBT6jS6chqHPs04Lo6qhaTr9MhMvy1nsYv8Xel8o8VgIFAqFqFYHz/T/Byn/C0wqgfeCSjuNAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/f8e1eb3622c86ae39b3f94692664644d/b6c86/spider-1.png","srcSet":"/static/f8e1eb3622c86ae39b3f94692664644d/5d44a/spider-1.png 175w,\n/static/f8e1eb3622c86ae39b3f94692664644d/4a1c0/spider-1.png 350w,\n/static/f8e1eb3622c86ae39b3f94692664644d/b6c86/spider-1.png 700w,\n/static/f8e1eb3622c86ae39b3f94692664644d/d72e9/spider-1.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/f8e1eb3622c86ae39b3f94692664644d/f9c03/spider-1.avif 175w,\n/static/f8e1eb3622c86ae39b3f94692664644d/00cca/spider-1.avif 350w,\n/static/f8e1eb3622c86ae39b3f94692664644d/80c82/spider-1.avif 700w,\n/static/f8e1eb3622c86ae39b3f94692664644d/10aea/spider-1.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/f8e1eb3622c86ae39b3f94692664644d/25cde/spider-1.webp 175w,\n/static/f8e1eb3622c86ae39b3f94692664644d/2f5a2/spider-1.webp 350w,\n/static/f8e1eb3622c86ae39b3f94692664644d/a84ee/spider-1.webp 700w,\n/static/f8e1eb3622c86ae39b3f94692664644d/3af47/spider-1.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":545}}},"tech":["Vision Language Models"],"github":"https://github.com/csebuetnlp/IllusionVQA","external":"https://illusionvqa.github.io/"},"html":"<p>The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially degrade the performance of GeminiPro on the localization task. Tangentially, we discover a potential weakness in the ICL capabilities of VLMs: they fail to locate optical illusions even when the correct answer is in the context window as a few-shot example.</p>"}},{"node":{"frontmatter":{"pos":"Machine Learning in Structural Biology, Workshop at the 38th Conference on Neural Information Processing Systems (MLSB @ NeurIPS 2024)","title":"RNA-DCGen: Dual Constrained RNA Sequence Generation with LLM-Attack","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABwElEQVQoz02SS24TQRCGfSxuwFWQOBFixw1gywqUdaQgZPKwYmxnPM6Mp7vn1T3Vrw/NGEJKqu7dp6qv/lUm869SvnSMkbYbGEaHDwkfIiEkhtEy2rkd3gde10zJObPKGXIKkAPBC10/YtoRVRXYuiDabgH3fY9uLftCYa3DOcHpkvH0G3fez1Ms4NX8+NGgn7f09QHbVYQA/aTZ1g90Vr1MUXUl9+UtMQdEIlXdUNWaqla4Sf4DU05oY1BK471nnlp1Fev1FaavX4AntWNT/MAHxySZybUM51+4/rioSfPKKUaiQPCRSQTvhdmO7B/ZvntDrB8ZJdIbgzQ7UrXBdyecQBJFp67x446YuABFBr5uPnBT3VAddlTlkXG0WGMor79hO0MMEXEO1xaM9ZbQV1jniTFx//4t9ud30rxpjKyG5PlcfeJ2f8XTw5q2fmYcLIMTDmpimq+ZEu35SG9KWjM79jiJTG6i+PKRdn93AaZ0cZgTqEajtCbEgPhI0zSUxwKj9eJvs91zeDrSNAoRwU2ebhTuTgPnTojpr8M5h6+zuFw9xEVyiHn5JSRM23NuNLrtmCQwySWbMYRl9ZQujD+Ls7KZWSlMagAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/a19207cfe4a1014d85851f444b40c159/52f3f/rnadcgen.png","srcSet":"/static/a19207cfe4a1014d85851f444b40c159/40ee4/rnadcgen.png 175w,\n/static/a19207cfe4a1014d85851f444b40c159/2cdd9/rnadcgen.png 350w,\n/static/a19207cfe4a1014d85851f444b40c159/52f3f/rnadcgen.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/a19207cfe4a1014d85851f444b40c159/92db5/rnadcgen.avif 175w,\n/static/a19207cfe4a1014d85851f444b40c159/e80d6/rnadcgen.avif 350w,\n/static/a19207cfe4a1014d85851f444b40c159/a1592/rnadcgen.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/a19207cfe4a1014d85851f444b40c159/94714/rnadcgen.webp 175w,\n/static/a19207cfe4a1014d85851f444b40c159/ab8da/rnadcgen.webp 350w,\n/static/a19207cfe4a1014d85851f444b40c159/7ee2e/rnadcgen.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":320}}},"tech":["LLM Attack","RNA Sequence Generation"],"github":"","external":"https://www.biorxiv.org/content/10.1101/2024.09.23.614570.abstract"},"html":"<p>Designing RNA sequences with specific properties is critical for developing personalized medications and therapeutics. While recent diffusion and flow-matching-based generative models have made strides in conditional sequence design, they face two key limitations: specialization for fixed constraint types, such as tertiary structures, and lack of flexibility in imposing additional conditions beyond the primary property of interest. To address these challenges, we introduce RNA-DCGen, a generalized framework for RNA sequence generation that is adaptable to any structural or functional properties through straightforward finetuning with an RNA language model (RNA-LM). Additionally, RNA-DCGen can enforce conditions on the generated sequences by fixing specific conserved regions. On RNA generation conditioned on RNA distance maps, RNA-DCGen generates sequences with an average R2 score of 0.625 compared to random sequences that score only 0.118 over 250 generations as judged by a separate more capable RNA-LM. When conditioned on RNA secondary structures, RNA-DCGen achieves an average F1 score of 0.4 against a random baseline of 0.006.</p>"}},{"node":{"frontmatter":{"pos":"arXiv Preprint","title":"Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAC7klEQVQ4y3WU22/jRBTG8/8LIZ54WhAvsGIlkFC7bAtpmvs9NNfNzU7s2LEdx07sJE5L2u4PzXi9baE8fJqx58z3nducVM3c8Rwlzac48yjOvXj9HxRUl7wisKKib7n6aJBXXVL15R6BmrmnYR0ozX35XTXCVyFEK4tAkgnRgromO7H5pdjmsqeRSjwTJJnRkouOKo0TodeQnThcjy15Lzd1KM7WvPktzdtM7YlQeCcOzxtDqRp7vfsXBOGODzcKZ42R9PCsPuR9a8y7XIvz5uhlyHXrQFnbSPLE6+dIhGtGKPMm9gWZb/+LzYuQxVqYPXn3RUjs5Xdsm1PWXI0davqGmWnT6AxQbE/a/IewOHsZriSyDlTNPVUroqraDFWdzkhhunRZuS75bAYv3NO0o5gwqZ64XJp7sftWRMM+UrOOVCYmXUWjqxrYQcQxOrD1PT4Bs/mcb77+itMnJIfM4c36JPNRNkIyE4eKuafwUaPYV8gP5nQVnXq5SLVSxt9F3D88cH93S3D3wMA9Mt2e6LuRTElKFOGH91lZ4RvDp6FatI0NqmkzV6boC4PD6YHvv3tD+vIC7/ZRXp74t3Qd4cSO8iKkYsRdkMpNHL59dy47f6Yb9DttLMuS4fz89id8b836DoZOyHxzpO8eKX0mqL7SCSlR9rPmmJyywgt2NOt1giDAPT6SrbdZeCE990hxsYu9MZ76svoKUt1VRMc50Lb3hH8/st6GRKc4rJ5/z40T0XYOCLsEnWR1DhLP96mmGVDVPFrWjvLcI93TKKourWVI09jSWu7kvmEENM2Q+mLD9UAn09coKQ6F8ZKr3kzuhXgqPVjwY7rK9dimpG340FY/P3iHP/o62elKpuOiM+Oyq5FX16QHBn/2F9LuemTJlJX1IC6KeOS//zUhMzTJTVeUNV++FiEkzgTE2fVoKVcxQMSoEhA28bhbyycriyIUfi215c/nTS4MsmNbklf0jSQUrSVGVyKSjLqETBD+A3dQv6MBebMHAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/7c34d9a9206ff432a0412f2c627ad986/e9447/pipeline.png","srcSet":"/static/7c34d9a9206ff432a0412f2c627ad986/25f13/pipeline.png 175w,\n/static/7c34d9a9206ff432a0412f2c627ad986/ee9c6/pipeline.png 350w,\n/static/7c34d9a9206ff432a0412f2c627ad986/e9447/pipeline.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/7c34d9a9206ff432a0412f2c627ad986/fdd97/pipeline.avif 175w,\n/static/7c34d9a9206ff432a0412f2c627ad986/30ec6/pipeline.avif 350w,\n/static/7c34d9a9206ff432a0412f2c627ad986/871c7/pipeline.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/7c34d9a9206ff432a0412f2c627ad986/09ff3/pipeline.webp 175w,\n/static/7c34d9a9206ff432a0412f2c627ad986/0edf5/pipeline.webp 350w,\n/static/7c34d9a9206ff432a0412f2c627ad986/3f093/pipeline.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":579}}},"tech":["Low Resrouce NLP","Large Language Models"],"github":"","external":"https://arxiv.org/abs/2407.00416"},"html":"<p>Each new generation of English-oriented Large Language Models (LLMs) exhibits enhanced cross-lingual transfer capabilities and significantly outperforms older LLMs on low-resource languages. This prompts the question: Is there a need for LLMs dedicated to a particular low-resource language? We aim to explore this question for Bengali, a low-to-moderate resource Indo-Aryan language native to the Bengal region of South Asia.\nWe compare the performance of open-weight and closed-source LLMs such as LLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse set of Bengali downstream tasks, including translation, summarization, paraphrasing, question-answering, and natural language inference. Our findings reveal that while LLMs generally excel in reasoning tasks, their performance in tasks requiring Bengali script generation is inconsistent. Key challenges include inefficient tokenization of Bengali script by existing LLMs, leading to increased computational costs and potential performance degradation. Additionally, we highlight biases in machine-translated datasets commonly used for Bengali NLP tasks. We conclude that there is a significant need for a Bengali-oriented LLM, but the field currently lacks the high-quality pretraining and instruction-tuning datasets necessary to develop a highly effective model.</p>"}},{"node":{"frontmatter":{"pos":"2023 IEEE SPS Video and Image Processing Cup at International Conference on Image Processing (ICIP)","title":"Ophthalmic Biomarker Detection Using Ensembled Vision Transformers","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAA7DAAAOwwHHb6hkAAADaElEQVQ4y4WVCVMbRxCF9f9/RqqcVAoDdopUYhAYsE1kgZygA0noQtdKe0h77+z9pXYBIQmRdFXXbO1Ov3nd+7qnwBuWpmm+ep7LRFGRFJUwCDa+7bLCNkC2PntmpqFzfVvjtnGH8LyNPduxK8D1Ddunx0nCbDpFns/fzGJ9LexiFUURvhD5mtlisUDX9fw5ex+GIUkcZ4GvYgvbjAzLyn0iSeiGgWVZzGYzZFl+fJZlFrqOtlwSJckjszXGK4ZiNMQf9FHrNbxBn2DQJzKMFePkKTiS5wQPA8aVG5R6jfBhQKipm4ChruOWrmgc7lPP/OA98+IxfqP2ePpTFonrsvx6SevDAde/vMv3dY9+w7opk2YleP4pkevgVa7RLs+ZfT5lXDzGKV0hBr2NcmQszb9/IJ2eUN3fo/P7EfJZEadeXaW9qmEsBKkQ2JpGGgYE8hyv181LIYYP+KMh0XSC1+1k4sTRl4isJL5PGoUvKadbUjEdB9e2sf/5gXp+RvPDQZ7a7d6v6F8vEeUSot9jaZqETypYl86WDtNcEoaiYJb+Ylo8pvzzO5ofD5meniCdFQnKJaxaFcMwHn/UloY3AJM1puFCw23dsazXMBp1gu49Ub+HuG+T+v4KIHnyN3UoPBfXsVBVBWk6QdVUlrqe60+azVAUGdu2MI0lpPGrzskZWl6EbIRoVkRnpDKWLR5Um85cx/N84jjKO8N0BBPVZao6dEcaw5mBZPiMlh5+mLzUUDMDmlJC8XuHdx8vOPzjO5/uFY660kYn2CKmKcUcntzw0/4pe0ffOCi3ed+eEK0DRnFCcyIoNWT+/HbHabnLdXvJXA9zuPV+7c58zitDji5u+XTV5MutxEh72Vd4LGHK3AjpyyE39THtkcVACXH9mG0VaFaYg1YaE2pdNY8x3WhNh+tTJgloPTRQLZn/s960w1gdvq3DvEa+yWX1M5X7a4IgwPM8wiAkCiOiMMb3fXzhYzg6l9VzKp0yKUle5Gdihe0hmcRJHuS6LsITTJUJd/06jV6dhaHlh2QuhL9z0BZ2Td5168sdvlQvcl846qbueD3p37wCspRN08RxnHy4Zm7b9qrltmM2AHfdE1nwcDik1WrRbt/TbrfpdDoMBoPV1bAro8J/XTz5/DNNxuNx7kKIDXa7AP8Fi4osanSSDCYAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/22980c33251778c93ffb2a41b581c4bf/666d4/ensemble.png","srcSet":"/static/22980c33251778c93ffb2a41b581c4bf/4a951/ensemble.png 155w,\n/static/22980c33251778c93ffb2a41b581c4bf/d3509/ensemble.png 310w,\n/static/22980c33251778c93ffb2a41b581c4bf/666d4/ensemble.png 619w","sizes":"(min-width: 619px) 619px, 100vw"},"sources":[{"srcSet":"/static/22980c33251778c93ffb2a41b581c4bf/ceb3c/ensemble.avif 155w,\n/static/22980c33251778c93ffb2a41b581c4bf/56ddc/ensemble.avif 310w,\n/static/22980c33251778c93ffb2a41b581c4bf/bcb2e/ensemble.avif 619w","type":"image/avif","sizes":"(min-width: 619px) 619px, 100vw"},{"srcSet":"/static/22980c33251778c93ffb2a41b581c4bf/3239d/ensemble.webp 155w,\n/static/22980c33251778c93ffb2a41b581c4bf/13b4d/ensemble.webp 310w,\n/static/22980c33251778c93ffb2a41b581c4bf/8a6f6/ensemble.webp 619w","type":"image/webp","sizes":"(min-width: 619px) 619px, 100vw"}]},"width":700,"height":840.2261712439418}}},"tech":["Vision Transformer","Retinal Biomarker Detection"],"github":"","external":"https://arxiv.org/abs/2310.14005"},"html":"<p>This report outlines our approach in the IEEE SPS VIP Cup 2023: Ophthalmic Biomarker Detection competition. Our primary objective in this competition was to identify biomarkers from Optical Coherence Tomography (OCT) images obtained from a diverse range of patients. Using robust augmentations and 5-fold cross-validation, we trained two vision transformer-based models: MaxViT and EVA-02, and ensembled them at inference time. We find MaxViT's use of convolution layers followed by strided attention to be better suited for the detection of local features while EVA-02's use of normal attention mechanism and knowledge distillation is better for detecting global features. Ours was the best-performing solution in the competition, achieving a patient-wise F1 score of 0.814 in the first phase and 0.8527 in the second and final phase of VIP Cup 2023, scoring 3.8% higher than the next-best solution.</p>"}},{"node":{"frontmatter":{"pos":"arXiv Preprint","title":"Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACPUlEQVQoz12Sy08TURSH+wdqDG7UAMH3QgMSsYC0PAJoYQFGExYmJiauNK6MBhfGBF+gobEUaAu0BXm10ymdtjPtvO98ZqZpajzJvTc5J/fL7/zOCXmex//RyrUPeMLFNAw8T2AYOopSoV5T0DSt/YM2J+RflmWhqipCCBzHCQq65WE54DiColxHqVZRtQaa1qB8Vg1ypmnSMDxc0RES8gG5XI5kMkmxWKTRbIKwOC6cUZDOaGp1UtlTXFunLBdoalVqtRq5ozJgs/enRK1eB88J+gn51LZc//VVmrmXeOkZ9M1HOOk5nPRjKutTHH8dR1qdoJmcRWRi2KkYYmcee2sSs/il0/K/EQB3lrATYYxEFLEVxUyMYW1EKKwMoqyNYCQe4mxG8VITmBsRRDKMefKpBXRdl3w+TyqVolKpoKkqll5F6DL7uThWU6Yi7XJ6kqFRLZJJ/8L2c6VdZCmL3ZA4PEjgWI0W0DCMAOj76HuoKEpQOJR0skdS4Mt6WkIIg/2CzrEkYzse8YwEmKQP6pyUyp2h+EAfls1mkSQpWIFSzeXpssObNfi85TH/ThA/gNjrAm+/qSxvwMJ7QTwPCx8EH5PguK01C4Zi2zY+2LJshHBZ+fGbxaVXzD15QWTmGbHF59wbjdE/NEE4Ok/f7TADD6a41R9lcHiam3fH2E7vdTz025RlOdhFH/5z9Tuz05OMDA8xdH+A8cgo16/20NfbzUD/HS52naen+zI3rvXSfeUSXRfOkdreDoB/Af6Qxl0+GVSJAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/c71162375bee27200152fa98fb170571/fb5fd/audio_wav2vec2_arch.png","srcSet":"/static/c71162375bee27200152fa98fb170571/1f575/audio_wav2vec2_arch.png 175w,\n/static/c71162375bee27200152fa98fb170571/6d683/audio_wav2vec2_arch.png 350w,\n/static/c71162375bee27200152fa98fb170571/fb5fd/audio_wav2vec2_arch.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/c71162375bee27200152fa98fb170571/53ff3/audio_wav2vec2_arch.avif 175w,\n/static/c71162375bee27200152fa98fb170571/9da12/audio_wav2vec2_arch.avif 350w,\n/static/c71162375bee27200152fa98fb170571/56c38/audio_wav2vec2_arch.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/c71162375bee27200152fa98fb170571/c654f/audio_wav2vec2_arch.webp 175w,\n/static/c71162375bee27200152fa98fb170571/a9207/audio_wav2vec2_arch.webp 350w,\n/static/c71162375bee27200152fa98fb170571/6de28/audio_wav2vec2_arch.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":365}}},"tech":["Automatic Speech Recognition","Bengali Common Voice Speech Dataset"],"github":"https://github.com/Patchwork53/DLSprint2022-Champion","external":"https://arxiv.org/abs/2209.06581"},"html":"<p>Speech is inherently continuous, where discrete words, phonemes and other units are not clearly segmented, and so speech recognition has been an active research problem for decades. In this work we have fine-tuned wav2vec 2.0 to recognize and transcribe Bengali speech -- training it on the Bengali Common Voice Speech Dataset. After training for 71 epochs, on a training set consisting of 36919 mp3 files, we achieved a training loss of 0.3172 and WER of 0.2524 on a validation set of size 7,747. Using a 5-gram language model, the Levenshtein Distance was 2.6446 on a test set of size 7,747. Then the training set and validation set were combined, shuffled and split into 85-15 ratio. Training for 7 more epochs on this combined dataset yielded an improved Levenshtein Distance of 2.60753 on the test set. Our model was the best performing one, achieving a Levenshtein Distance of 6.234 on a hidden dataset, which was 1.1049 units lower than other competing submissions.</p>"}},{"node":{"frontmatter":{"pos":"arXiv Preprint","title":"Bangla grammatical error detection using t5 transformer model","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABaUlEQVQoz1WR226jQBBE+f/PSlbKRlrJa3DWiRlguBqMYzyYMXef1UDykJZKXWqpWt1V1uPxYJomTDf13ed5XrhBP7V0U0Otr+hO0Q6aTt+p84LrF1Re0N5uWEZYVRV5nv/A5XJBa804j4g0YCMcROaxcR38POT2qagOR4q3iHwXLrw5KaxxHInjGCklaZqQJCvM0qZp6KeR+Bhw8G1cf4cbOGSF5F5rklNEmHnEWUBw9GhuCqvve87nM3VdIzyPp+dnzGwYBrquY5pnoqPHQW5597a4oU16kuhrwz74YB/sEbGL7TooVWEZ4erZA8fe8Pr7F182Yq6/9y1eJvkrdoh0fTk6JbSNJohs/n28Yr+94Pp/qK/lutCg0ZpQSsqiWEL6vnCcJ5JUEPhbhLAJpUN5jhnaniITJNGeKNiTxe/c6wrLiE0ASqmll2W5cBOU8dDUZ3NBVuHCpQrRw339oB+Zxxlmw6dl9h+0ug6YBXxeZwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/9f0046a9c6bf7878f60ac5fd906d9aa3/a515b/t5.png","srcSet":"/static/9f0046a9c6bf7878f60ac5fd906d9aa3/79643/t5.png 175w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/a68a2/t5.png 350w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/a515b/t5.png 700w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/f831e/t5.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/9f0046a9c6bf7878f60ac5fd906d9aa3/25982/t5.avif 175w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/b447a/t5.avif 350w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/3bf09/t5.avif 700w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/4c8b1/t5.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/9f0046a9c6bf7878f60ac5fd906d9aa3/aae4c/t5.webp 175w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/1fd3c/t5.webp 350w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/15b88/t5.webp 700w,\n/static/9f0046a9c6bf7878f60ac5fd906d9aa3/cc7b5/t5.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":259}}},"tech":["Bengali Grammatical Error Detection","T5 Transformer"],"github":"https://github.com/Patchwork53/EEEDAY-Dathathon2023-1st-Runners-Up","external":"https://arxiv.org/abs/2303.10612"},"html":"<p>This paper presents a method for detecting grammatical errors in Bangla using a Text-to-Text Transfer Transformer (T5) Language Model, using the small variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were bracketed by the dedicated demarcation symbol. The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection. Our experiments show that the T5 model can achieve low Levenshtein Distance in detecting grammatical errors in Bangla, but post-processing is essential to achieve optimal performance. The final average Levenshtein Distance after post-processing the output of the fine-tuned model was 1.0394 on a test set of 5000 sentences. This paper also presents a detailed analysis of the errors detected by the model and discusses the challenges of adapting a translation model for grammar. Our approach can be extended to other languages, demonstrating the potential of T5 models for detecting grammatical errors in a wide range of languages.</p>"}},{"node":{"frontmatter":{"pos":"arXiv Preprint","title":"Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for Accurate Bangla Sign Language Recognition","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABuElEQVQoz3WSy2sTURTG828JLtyKOzcuFP8BMSCia0EXJYiP0tRaq7HV+lgoqJU4SWlNC6nJEI3RtrEtmNrJo8kkQ5yk87p3fjIdxQnigXMv9zw+vnPuF/N9n6gH5nketZ1tbMdF+pL/298eft+xaFrKMNjt6CQuXabXHx6+zYZDJa2xvtSinNX4nKnzZbWOJwJI8IREyBA85jo2rm3hCUGYBtfzWCuWMAc2m+08z5eeMD55h5l747x7+oKH01N8UpaRgxpgj3CO1bQGc7PzpJKT5Ms7qFtdVtUtUtcTePaAK5kkR6ducWJ+gsTKKb7+SHG/cI69joJrFNltGbxda1LY6IYM26ZDWt1GyeTIlwtk1UXUjTKvXi5gHZhcXXzEkduPOT49y5hyhuJ6irvLcT5UF+h0SqTVfY7Fc1xMVhBSju4wGNkVDmbf4P2b1wjH4ubKBCfnrnH22RiJ7Gk+Vh8wk4tT0xTol7AOhrR6NsZPJ2QYHAGyEAIpwx/V6xo3LpzHHFgYw+80+xV0c5NGt4re22Wv/Q3P2kfYOr50R3cYlUtUPtL3sd1/JaNpLZqNdmQmDmv/9P0Cy9JChtvZs6sAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/b693afc5f1050efc2a431304698b24de/56edf/3dcnn.png","srcSet":"/static/b693afc5f1050efc2a431304698b24de/ce9c2/3dcnn.png 175w,\n/static/b693afc5f1050efc2a431304698b24de/97ce3/3dcnn.png 350w,\n/static/b693afc5f1050efc2a431304698b24de/56edf/3dcnn.png 700w,\n/static/b693afc5f1050efc2a431304698b24de/f61b5/3dcnn.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/b693afc5f1050efc2a431304698b24de/c4411/3dcnn.avif 175w,\n/static/b693afc5f1050efc2a431304698b24de/15b72/3dcnn.avif 350w,\n/static/b693afc5f1050efc2a431304698b24de/e0ddb/3dcnn.avif 700w,\n/static/b693afc5f1050efc2a431304698b24de/82d0f/3dcnn.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/b693afc5f1050efc2a431304698b24de/36bb8/3dcnn.webp 175w,\n/static/b693afc5f1050efc2a431304698b24de/434b8/3dcnn.webp 350w,\n/static/b693afc5f1050efc2a431304698b24de/4f25c/3dcnn.webp 700w,\n/static/b693afc5f1050efc2a431304698b24de/836bd/3dcnn.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":280}}},"tech":["Bangla Sign Language Recognition","3D CNN","ST"],"github":"https://github.com/Patchwork53/BdSL40_Dataset_AI_for_Bangla_2.0_Honorable_Mention","external":"https://arxiv.org/abs/2401.12210"},"html":"<p>Recent advances in Deep Learning and Computer Vision have been successfully leveraged to serve marginalized communities in various contexts. One such area is Sign Language - a primary means of communication for the deaf community. However, so far, the bulk of research efforts and investments have gone into American Sign Language, and research activity into low-resource sign languages - especially Bangla Sign Language - has lagged significantly. In this research paper, we present a new word-level Bangla Sign Language dataset - BdSL40 - consisting of 611 videos over 40 words, along with two different approaches: one with a 3D Convolutional Neural Network model and another with a novel Graph Neural Network approach for the classification of BdSL40 dataset. This is the first study on word-level BdSL recognition, and the dataset was transcribed from Indian Sign Language (ISL) using the Bangla Sign Language Dictionary (1997). The proposed GNN model achieved an F1 score of 89%. The study highlights the significant lexical and semantic similarity between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature. We release the dataset and source code to stimulate further research.</p>"}}]}}}